{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark tutorial for python\n",
    "\n",
    "Xiangnan YUE\n",
    "\n",
    "The following serves as my naive experiments with Spark, where you can find on the Spark website a similar one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and test\n",
    "\n",
    "You need first to install spark, then to use spark for python, you will need pyspark.\n",
    "To use pyspark, you can type 'pyspark' in the bshell and it leads to the python shell\n",
    "To use pyspark in the Jupyter, you will need to add the following to the \".bash_profile\" (for Mac)\n",
    "\n",
    "> export PYSPARK_DRIVER_PYTHON=jupyter\n",
    ">\n",
    "> export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "\n",
    "reopen the terminal and type\n",
    "\n",
    "> pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following codes can also be used to start the pyspark, findspark package is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "#import pyspark\n",
    "#sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to import the sc, normaly we Cannot run multiple SparkContexts at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /anaconda/lib/python3.6/site-packages/IPython/utils/py3compat.py:186 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fa4b448683f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pylab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[2]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pyspark tutorial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/ethylene_methane.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdist_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /anaconda/lib/python3.6/site-packages/IPython/utils/py3compat.py:186 "
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "#from pyspark import SparkContext \n",
    "#sc = SparkContext('local[2]', 'pyspark tutorial')\n",
    "\n",
    "filename = './data/ethylene_methane.txt'\n",
    "dist_data = sc.textFile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4178505"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.count() # Number of rows in this DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### too large a file, use a sample playme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (seconds), CO conc (ppm), Ethylene conc (ppm), sensor readings (16 channels) \n",
      "0.00    0.00    0.00    -50.85  -1.95   -41.82  1.30    -4.07   -28.73  -13.49  -3.25   55139.95 50669.50 9626.26 9762.62 24544.02 21420.68 7650.61 6928.42\n",
      "0.01    0.00    0.00    -49.40  -5.53   -42.78  0.49    3.58    -34.55  -9.59   5.37    54395.77 50046.91 9433.20 9591.21 24137.13 20930.33 7498.79 6800.66\n",
      "0.01    0.00    0.00    -40.04  -16.09  -27.59  0.00    -7.16   -42.14  -12.52  -5.86   53960.02 49299.30 9324.40 9449.81 23628.90 20504.94 7369.67 6697.47\n",
      "0.03    0.00    0.00    -47.14  -10.57  -32.28  4.40    -11.22  -37.94  -7.16   -1.14   53047.71 48907.00 9170.64 9305.58 23101.66 20101.42 7285.13 6578.52\n",
      "0.04    0.00    0.00    -33.58  -20.79  -33.25  6.03    3.42    -34.22  -14.46  8.31    52700.28 48330.96 9073.64 9163.47 22689.54 19694.07 7156.74 6468.32\n",
      "0.05    0.00    0.00    -48.59  -11.54  -36.16  6.03    0.33    -29.05  -16.74  -1.14   51910.52 47609.00 8982.88 9021.08 22159.12 19332.57 7067.61 6385.31\n",
      "0.06    0.00    0.00    -48.27  -9.11   -31.31  5.37    -7.97   -30.34  -8.62   7.00    51458.44 47047.36 8860.51 8966.48 21931.81 19027.69 6976.13 6300.97\n",
      "0.07    0.00    0.00    -47.14  -4.56   -16.57  4.40    -2.28   -24.35  -13.17  -6.34   51043.22 46757.50 8776.93 8868.04 21504.46 18780.21 6900.01 6248.81\n",
      "0.07    0.00    0.00    -42.30  -2.77   -20.79  23.98   -2.12   -27.59  -6.83   -0.81   50353.63 46196.84 8709.46 8775.21 21124.62 18439.18 6827.34 6183.33\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -10 data/ethylene_CO.txt > playme.txt\n",
    "cat playme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.csv??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the playme file as an example to play with, faster\n",
    "textFile = spark.read.text(\"playme.txt\")\n",
    "type(textFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play with pyspark.sql.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['value']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Time (seconds), C...|\n",
      "|0.00    0.00    0...|\n",
      "|0.01    0.00    0...|\n",
      "|0.01    0.00    0...|\n",
      "|0.03    0.00    0...|\n",
      "|0.04    0.00    0...|\n",
      "|0.05    0.00    0...|\n",
      "|0.06    0.00    0...|\n",
      "|0.07    0.00    0...|\n",
      "|0.07    0.00    0...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='Time (seconds), CO conc (ppm), Ethylene conc (ppm), sensor readings (16 channels) '),\n",
       " Row(value='0.00    0.00    0.00    -50.85  -1.95   -41.82  1.30    -4.07   -28.73  -13.49  -3.25   55139.95 50669.50 9626.26 9762.62 24544.02 21420.68 7650.61 6928.42')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head = textFile.first().value.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time (seconds)',\n",
       " ' CO conc (ppm)',\n",
       " ' Ethylene conc (ppm)',\n",
       " ' sensor readings (16 channels) ']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='Time (seconds), CO conc (ppm), Ethylene conc (ppm), sensor readings (16 channels) ')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftf = textFile.first()\n",
    "ftf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use a filter but can only be used on rdd, not a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "condition should be string or Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d657e41654ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: condition should be string or Column"
     ]
    }
   ],
   "source": [
    "textFile = textFile.filter(lambda line: line[0:4] != 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
